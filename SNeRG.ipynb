{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries: Clone SNeRG repo"
      ],
      "metadata": {
        "id": "Ki19EiSby3vF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM_88u6AxTZ6",
        "outputId": "8a288365-003d-4c39-8cde-734a78121298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'snerg'...\n",
            "remote: Enumerating objects: 88, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 88 (delta 9), reused 21 (delta 5), pack-reused 62\u001b[K\n",
            "Unpacking objects: 100% (88/88), 168.24 MiB | 12.02 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Clone SNeRG repository\n",
        "!git clone https://github.com/simicvm/snerg.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "!pip install -r /content/snerg/requirements.txt"
      ],
      "metadata": {
        "id": "ypA6k4LXx4q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test whether packages have been installed successfully\n",
        "import pkgutil\n",
        "assert pkgutil.find_loader('jaxlib') is not None"
      ],
      "metadata": {
        "id": "K8WhuDhAyiCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Jaxlib for CUDA version"
      ],
      "metadata": {
        "id": "hiGIStm4zH9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6mGcSn1zRGk",
        "outputId": "21d4fe59-f3f9-4e13-ef97-34d37a7743cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 29 19:05:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade jax[cuda12_pip]==0.4.6 jaxlib[cuda12_pip]==0.4.6 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLYu1ufNzOs2",
        "outputId": "053b415d-6832-4d5f-efb6-b2fcc774fd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda12_pip]==0.4.6 in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Requirement already satisfied: jaxlib[cuda12_pip]==0.4.6 in /usr/local/lib/python3.10/dist-packages (0.4.6)\n",
            "Collecting jaxlib[cuda12_pip]==0.4.6\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.6%2Bcuda11.cudnn86-cp310-cp310-manylinux2014_x86_64.whl (147.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.6/147.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: jax 0.4.6 does not provide the extra 'cuda12_pip'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.6) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.6) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax[cuda12_pip]==0.4.6) (1.10.1)\n",
            "\u001b[33mWARNING: jaxlib 0.4.6+cuda11.cudnn86 does not provide the extra 'cuda12_pip'\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.6\n",
            "    Uninstalling jaxlib-0.4.6:\n",
            "      Successfully uninstalled jaxlib-0.4.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.2.6 requires jax>=0.4.9, but you have jax 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jaxlib-0.4.6+cuda11.cudnn86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify install"
      ],
      "metadata": {
        "id": "xAkFE6MW1Wp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd snerg; python -m train \\\n",
        "  --data_dir=/content/snerg/example_data \\\n",
        "  --train_dir=/content/checkpoints \\\n",
        "  --max_steps=5 \\\n",
        "  --factor=2 \\\n",
        "  --batch_size=512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHLbwJcg1aaT",
        "outputId": "5046f125-e233-4918-b943-68c02b958ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "2023-06-29 19:08:08.450073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/flax/struct.py:132: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
            "I0629 19:08:10.586497 140300097906496 xla_bridge.py:166] Remote TPU is not linked into jax; skipping remote TPU.\n",
            "I0629 19:08:10.586737 140300097906496 xla_bridge.py:413] Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'\n",
            "I0629 19:08:10.690046 140300097906496 xla_bridge.py:413] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Host CUDA Interpreter\n",
            "I0629 19:08:10.691615 140300097906496 xla_bridge.py:413] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
            "I0629 19:08:10.691757 140300097906496 xla_bridge.py:413] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:600: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:613: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/flax/optim/base.py:49: DeprecationWarning: Use `optax` instead of `flax.optim`. Refer to the update guide https://flax.readthedocs.io/en/latest/howtos/optax_update_guide.html for detailed instructions.\n",
            "  warnings.warn(\n",
            "I0629 19:08:34.154186 140300097906496 checkpoints.py:425] Found no checkpoint files in /content/tmp/snerg_test with prefix checkpoint_\n",
            "/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:600: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n",
            "2023-06-29 19:08:34.269484: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "/usr/local/lib/python3.10/dist-packages/flax/optim/base.py:90: FutureWarning: jax.tree_flatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_flatten instead.\n",
            "  params_flat, treedef = jax.tree_flatten(params)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/optim/base.py:97: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
            "  new_params = jax.tree_unflatten(treedef, new_params_flat)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/optim/base.py:98: FutureWarning: jax.tree_unflatten is deprecated, and will be removed in a future release. Use jax.tree_util.tree_unflatten instead.\n",
            "  new_param_states = jax.tree_unflatten(treedef, new_states_flat)\n",
            "/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/mlir.py:711: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512,3]), ShapedArray(float32[512,3]), ShapedArray(float32[512,3]), ShapedArray(float32[512,3]).\n",
            "See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.\n",
            "  warnings.warn(f\"Some donated buffers were not usable: {', '.join(unused_donations)}.\\n{msg}\")\n",
            "I0629 19:08:51.201763 140300097906496 checkpoints.py:317] Saving checkpoint at step: 5\n",
            "I0629 19:08:51.228931 140300097906496 checkpoints.py:278] Saved checkpoint at /content/tmp/snerg_test/checkpoint_5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m bake \\\n",
        "  --data_dir=/content/snerg/example_data \\\n",
        "  --train_dir=/content/checkpoints \\\n",
        "  --config=configs/CONFIG_YOU_LIKE"
      ],
      "metadata": {
        "id": "Us5axqpsmurl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}